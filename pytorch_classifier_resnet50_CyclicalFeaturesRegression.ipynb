{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72090e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_erosion\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf5efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = \"/home/bandyadkas/cellcyle/data/\"\n",
    "images = \"/mnt/efs/woods_hole/bbbc_cellcycle/model_data/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05b0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "        'train': transforms.Compose([\n",
    "            #transforms.Resize([224,224]), # Resizing the image as the VGG only take 224 x 244 as input size\n",
    "            #transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
    "            #TODO if it is needed, add the random crop\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            #transforms.Resize([224,224]),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=(0), std=(1))\n",
    "        ])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69aba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32266\n"
     ]
    }
   ],
   "source": [
    "all_images_raw = datasets.ImageFolder(images,transform=transform['train'])\n",
    "print(len(all_images_raw))\n",
    "#all_images = torchvision.datasets.DatasetFolder(\n",
    "#    \"/mnt/efs/woods_hole/bbbc_cellcycle/CellCycle/\",\n",
    "#    imageio.imread, (\"merged.jpg\"), transform=transform)\n",
    "#print(len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdac2a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anaphase': 0, 'G1': 1, 'G2': 2, 'Metaphase': 3, 'Prophase': 4, 'S': 5, 'Telophase': 6}\n",
      "{'G1': 0, 'S': 1, 'G2': 2, 'Prophase': 3, 'Metaphase': 4, 'Anaphase': 5, 'Telophase': 6}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "print(all_images_raw.class_to_idx)\n",
    "new_classIds = {'G1': 0, 'S': 1, 'G2': 2, 'Prophase': 3, 'Metaphase': 4, 'Anaphase': 5, 'Telophase': 6}\n",
    "all_images_new_classIDs = copy.deepcopy(all_images_raw)\n",
    "all_images_new_classIDs.class_to_idx = new_classIds\n",
    "print(all_images_new_classIDs.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe70aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sines = np.sin(2 * np.pi*np.array(all_images_new_classIDs.targets)/7)\n",
    "cosines = np.cos(2 * np.pi*np.array(all_images_new_classIDs.targets)/7)\n",
    "\n",
    "#print(np.shape(sines), np.shape(cosines))\n",
    "sine_cosine_targets = np.stack((sines,cosines),axis=1)\n",
    "#print(np.shape(sine_cosine_targets))\n",
    "\n",
    "\n",
    "all_images_circ_targets = copy.deepcopy(all_images_new_classIDs)\n",
    "all_images_circ_targets.targets = sine_cosine_targets\n",
    "\n",
    "#print(np.shape(all_images_raw.targets))\n",
    "#print(np.shape(all_images_new_classIDs.targets))\n",
    "#print(np.shape(all_images_circ_targets.targets))\n",
    "assert np.shape(all_images_circ_targets.targets)==(32266,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77069f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22586 4839 4841\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(all_images_circ_targets))\n",
    "val_size = int(0.15 * len(all_images_circ_targets))\n",
    "test_size = len(all_images_circ_targets) - (train_size + val_size)\n",
    "print(train_size, val_size, test_size)\n",
    "assert train_size + val_size + test_size == len(all_images_circ_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b21067",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(all_images_circ_targets, [train_size, val_size, test_size])\n",
    "#train_set, val_set, test_set = torch.utils.data.random_split(all_images, [22538, 4829, 4831])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf02c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weights(subset,full_dataset):\n",
    "    ys = np.array([y for _, y in subset])\n",
    "    print(ys)\n",
    "    counts = np.bincount(ys)\n",
    "    label_weights = 1.0 / counts\n",
    "    weights = label_weights[ys]\n",
    "\n",
    "    print(\"Number of images per class:\")\n",
    "    for c, n, w in zip(full_dataset.classes, counts, label_weights):\n",
    "        print(f\"\\t{c}:\\tn={n}\\tweight={w}\")\n",
    "        \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_weights = _get_weights(train_set,all_images_circ_targets)\n",
    "train_sampler = WeightedRandomSampler(train_weights, len(train_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_circ_targets.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=8, drop_last=True, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_set, batch_size=8 , drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=8, drop_last=True, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up tensorboard\n",
    "#writer = SummaryWriter('/mnt/efs/woods_hole/bbbc_cellcycle/classify_cellCycle_bandyadka/runs/cellcycle_resnet50_CyclicalFeaturesRegression_2')\n",
    "\n",
    "writer = SummaryWriter('/home/bandyadkas/cellcyle/runs/cellcycle_resnet50_CyclicalFeaturesRegression_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45be449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9966af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = torchvision.models.resnet50(pretrained = False, progress  = True, num_classes=2)\n",
    "#loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchsummary import summary\n",
    "#resnet50_model_regression = nn.Sequential(*list(resnet50_model.children())[:-1])\n",
    "resnet50_model_regression = resnet50_model\n",
    "#resnet50_model_regression.add_module(nn.Sigmoid)\n",
    "summary(resnet50_model_regression.cuda(),(3,66,66))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(resnet50_model_regression.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet50_model_regression.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model,loss,train_dataloader):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "            \n",
    "        sines = [np.sin(2*np.pi*yi/7) for yi in np.array(y)]\n",
    "        cosines = [np.cos(2*np.pi*yi/7) for yi in np.array(y)]\n",
    "        sine_cosine_y = np.stack((sines,cosines),axis=-1)\n",
    "        \n",
    "        y = torch.from_numpy(sine_cosine_y)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        #print(x.shape, y.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y.float())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += l\n",
    "        num_batches += 1\n",
    "\n",
    "    return epoch_loss/num_batches\n",
    "\n",
    "def evaluate(model, loss, dataloader):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dataloader):\n",
    "            sines = [np.sin(2*np.pi*yi/7) for yi in np.array(y)]\n",
    "            cosines = [np.cos(2*np.pi*yi/7) for yi in np.array(y)]\n",
    "            sine_cosine_y = np.stack((sines,cosines),axis=-1)\n",
    "            \n",
    "            y = torch.from_numpy(sine_cosine_y)\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "\n",
    "            logits = model(x)\n",
    "            \n",
    "            val_loss = loss(logits,y.float())\n",
    "            \n",
    "            \n",
    "            #probs = torch.nn.Softmax(dim=1)(logits)\n",
    "            #predictions = torch.argmax(logits, dim=2)\n",
    "            #predictions1 = torch.argmax(logits, dim=1)\n",
    "            #print(np.shape(logits))\n",
    "            #print(np.shape(y))\n",
    "            #print(np.shape(predictions1))\n",
    "            #print(np.shape(predictions))\n",
    "\n",
    "            correct += int(torch.sum(logits == y).cpu().detach().numpy())\n",
    "            total += len(y)\n",
    "\n",
    "        accuracy = correct/total\n",
    "\n",
    "    return accuracy, val_loss, logits \n",
    "\n",
    "def validate(model,loss, validation_dataloader):\n",
    "    '''Evaluate prediction accuracy on the validation dataset.'''\n",
    "    \n",
    "    model.eval()\n",
    "    return evaluate(model,loss,validation_dataloader)\n",
    "\n",
    "def test(model,loss,test_dataloader):\n",
    "    '''Evaluate prediction accuracy on the test dataset.'''\n",
    "    \n",
    "    model.eval() \n",
    "    return evaluate(model, loss,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "epochs = 12\n",
    "for epoch in range(epochs+1):\n",
    "    while step < epoch: \n",
    "    \n",
    "        epoch_loss = train(resnet50_model_regression,loss_fn,train_loader)\n",
    "        print(f\"epoch {epoch}, training loss={epoch_loss}\")\n",
    "        \n",
    "        logits = validate(resnet50_model, loss_fn,val_loader)\n",
    "        print(logits)\n",
    "        \n",
    "    \n",
    "        validation_accuracy, validation_loss, logits = validate(resnet50_model_regression, loss_fn,val_loader)\n",
    "        print(f\"epoch {epoch}, validation accuracy={validation_accuracy}\")\n",
    "    \n",
    "        writer.add_scalar('Loss/train', epoch_loss.cpu().detach().numpy(),step)      \n",
    "        writer.add_scalar('Accuracy/validation', validation_accuracy,step)\n",
    "        writer.add_scalar('Loss/validation', validation_loss.cpu().detach().numpy(),step)\n",
    "        \n",
    "        while step < 51:\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': resnet50_model_regression.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(state, \"/home/bandyadkas/cellcyle/regressionmodel.pth\")\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model = torch.load(\"/mnt/efs/woods_hole/bbbc_cellcycle/classify_cellCycle_bandyadka/modelsave.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_loss = test(resnet50_model_regression,loss_fn,test_loader)\n",
    "print(f\"final test accuracy: {test_accuracy}\")\n",
    "writer.add_scalar('Accuracy/test', test_accuracy)\n",
    "writer.add_scalar('Loss/test', test_loss.cpu().detach().numpy(),step)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487304a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
